{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set other constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 2_000\n",
    "MAX_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            base_dir (str): Base directory to prepend to the image paths.\n",
    "            transform (callable, optional): Transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        # Strip any extra spaces from column names\n",
    "        self.data.columns = [col.strip() for col in self.data.columns]\n",
    "\n",
    "        # Build a mapping from font text to a class index\n",
    "        self.labels = sorted(self.data['text'].unique())\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and label from the CSV\n",
    "        img_path = self.data.iloc[idx]['img_path']\n",
    "        label_text = self.data.iloc[idx]['text']\n",
    "        \n",
    "        # Prepend the base directory to the image path\n",
    "        full_img_path = os.path.join(self.base_dir, img_path)\n",
    "        \n",
    "        # Load the image (ensure it exists)\n",
    "        image = Image.open(full_img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Convert text label to an integer class index\n",
    "        label = self.label_to_index[label_text]\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Use dataset-specific or ImageNet values\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could be useful in dataloading:\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/advanced/speed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct CSV file and the base directory\n",
    "csv_path = \"./dataset/data_2K_2.csv\"  # Update with your CSV file path\n",
    "base_dir = \"./dataset\"  # Base directory for your images\n",
    "# base_dir = \"./NEW_dataset/dataset\"  # Base directory for your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = FontDataset(csv_file=csv_path, base_dir=base_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the folder path\n",
    "# folder_path = \"./NEW_dataset/dataset/word_images/test\"  # Replace with your folder path\n",
    "\n",
    "# # Define the image extensions you want to count\n",
    "# image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp')\n",
    "\n",
    "# # Initialize a counter\n",
    "# image_count = 0\n",
    "\n",
    "# # Iterate through the files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.lower().endswith(image_extensions):\n",
    "#         image_count += 1\n",
    "\n",
    "# print(f\"Number of images in {folder_path}: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # Set this to the desired sample index within each class (0 = first sample, 9 = tenth, etc.)\n",
    "# sample_index = 222  # change this value as needed\n",
    "\n",
    "# # Define the unnormalization transformation.\n",
    "# unnormalize = transforms.Normalize(\n",
    "#     mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "#     std=[1/0.229, 1/0.224, 1/0.225]\n",
    "# )\n",
    "\n",
    "# # For example, if you want to show images from 20 classes:\n",
    "# samples_per_class = 1000\n",
    "# num_images_to_show = 20\n",
    "\n",
    "# # Create indices for 20 images, one per class.\n",
    "# indices = [i * samples_per_class + sample_index for i in range(num_images_to_show)]\n",
    "\n",
    "# # Create a figure with 4 rows and 5 columns (4*5 = 20 images).\n",
    "# fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for ax, idx in zip(axes, indices):\n",
    "#     image_tensor, label_idx = dataset[idx]\n",
    "#     label_text = dataset.labels[label_idx]\n",
    "    \n",
    "#     # Unnormalize and convert to a PIL image.\n",
    "#     image_unnorm = unnormalize(image_tensor)\n",
    "#     image_pil = transforms.ToPILImage()(image_unnorm)\n",
    "    \n",
    "#     ax.imshow(image_pil)\n",
    "#     ax.set_title(f\"Label: {label_text}\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # Set the desired font block index (0 = first font, 1 = second font, etc.)\n",
    "# desired_font_index =   499\n",
    "\n",
    "# # Define the unnormalization transformation.\n",
    "# unnormalize = transforms.Normalize(\n",
    "#     mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "#     std=[1/0.229, 1/0.224, 1/0.225]\n",
    "# )\n",
    "\n",
    "# # Define how many samples (words) per font (class) are in the dataset.\n",
    "# samples_per_class = 1000  # Adjust this value as per your dataset\n",
    "\n",
    "# # Calculate indices for the chosen font's block.\n",
    "# start_index = desired_font_index * samples_per_class\n",
    "# end_index = start_index + samples_per_class\n",
    "# indices = list(range(start_index, end_index))\n",
    "\n",
    "# # Create a figure with 2 rows and 5 columns.\n",
    "# fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for ax, idx in zip(axes, indices):\n",
    "#     image_tensor, label_idx = dataset[idx]\n",
    "#     # Here, we assume that dataset.labels contains the font label.\n",
    "#     label_text = dataset.labels[label_idx]\n",
    "    \n",
    "#     # Unnormalize and convert to a PIL image.\n",
    "#     image_unnorm = unnormalize(image_tensor)\n",
    "#     image_pil = transforms.ToPILImage()(image_unnorm)\n",
    "    \n",
    "#     ax.imshow(image_pil)\n",
    "#     ax.set_title(f\"Label: {label_text}\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3600000\n",
      "Val samples:   360000\n",
      "Test samples:  40000\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 1) Split dataset into 90% train and 10% temp\n",
    "# -------------------------------------------------\n",
    "train_df, temp_df = train_test_split(\n",
    "    data,\n",
    "    test_size=0.10,      # 10% of the entire dataset\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) From that temp (10%), split again:\n",
    "#    10% of temp => 1% of total is test\n",
    "#    the remaining 90% of temp => 9% of total is val\n",
    "# -------------------------------------------------\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.10,      # 10% of the 10% => 1% of the total\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Val samples:   {len(val_df)}\")\n",
    "print(f\"Test samples:  {len(test_df)}\")\n",
    "\n",
    "# Save to CSV if desired\n",
    "train_csv = \"train_split.csv\"\n",
    "val_csv   = \"val_split.csv\"\n",
    "test_csv  = \"test_split.csv\"\n",
    "\n",
    "train_df.to_csv(train_csv, index=False)\n",
    "val_df.to_csv(val_csv, index=False)\n",
    "test_df.to_csv(test_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3600000\n",
      "Validation dataset size: 360000\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation datasets using the new CSV splits\n",
    "train_dataset = FontDataset(csv_file=train_csv, base_dir=base_dir, transform=transform)\n",
    "val_dataset = FontDataset(csv_file=val_csv, base_dir=base_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet-18 for Arabic font classification\n",
    "class ArabicResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ArabicResNet18, self).__init__()\n",
    "        # Load standard ResNet-18 (no pre-trained weights)\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        # Modify the final fully connected layer to match the number of classes\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)  # Adjust based on your number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have 1000 classes (adjust as needed)\n",
    "num_classes = NUM_CLASSES  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ArabicResNet18(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)  # Learning rate: 2 × 10^-5\n",
    "\n",
    "# Learning rate scheduler (Exponential Decay: 10^(-1/90000))\n",
    "scheduler = ExponentialLR(optimizer, gamma=10**(-1/90_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ResNet: 1-1                            [-1, 2000]                --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
      "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
      "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
      "|    └─Sequential: 2-5                   [-1, 64, 56, 56]          --\n",
      "|    |    └─BasicBlock: 3-1              [-1, 64, 56, 56]          73,984\n",
      "|    |    └─BasicBlock: 3-2              [-1, 64, 56, 56]          73,984\n",
      "|    └─Sequential: 2-6                   [-1, 128, 28, 28]         --\n",
      "|    |    └─BasicBlock: 3-3              [-1, 128, 28, 28]         230,144\n",
      "|    |    └─BasicBlock: 3-4              [-1, 128, 28, 28]         295,424\n",
      "|    └─Sequential: 2-7                   [-1, 256, 14, 14]         --\n",
      "|    |    └─BasicBlock: 3-5              [-1, 256, 14, 14]         919,040\n",
      "|    |    └─BasicBlock: 3-6              [-1, 256, 14, 14]         1,180,672\n",
      "|    └─Sequential: 2-8                   [-1, 512, 7, 7]           --\n",
      "|    |    └─BasicBlock: 3-7              [-1, 512, 7, 7]           3,673,088\n",
      "|    |    └─BasicBlock: 3-8              [-1, 512, 7, 7]           4,720,640\n",
      "|    └─AdaptiveAvgPool2d: 2-9            [-1, 512, 1, 1]           --\n",
      "|    └─Linear: 2-10                      [-1, 2000]                1,026,000\n",
      "==========================================================================================\n",
      "Total params: 12,202,512\n",
      "Trainable params: 12,202,512\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.83\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 35.23\n",
      "Params size (MB): 46.55\n",
      "Estimated Total Size (MB): 82.36\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─ResNet: 1-1                            [-1, 2000]                --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 112, 112]        9,408\n",
       "|    └─BatchNorm2d: 2-2                  [-1, 64, 112, 112]        128\n",
       "|    └─ReLU: 2-3                         [-1, 64, 112, 112]        --\n",
       "|    └─MaxPool2d: 2-4                    [-1, 64, 56, 56]          --\n",
       "|    └─Sequential: 2-5                   [-1, 64, 56, 56]          --\n",
       "|    |    └─BasicBlock: 3-1              [-1, 64, 56, 56]          73,984\n",
       "|    |    └─BasicBlock: 3-2              [-1, 64, 56, 56]          73,984\n",
       "|    └─Sequential: 2-6                   [-1, 128, 28, 28]         --\n",
       "|    |    └─BasicBlock: 3-3              [-1, 128, 28, 28]         230,144\n",
       "|    |    └─BasicBlock: 3-4              [-1, 128, 28, 28]         295,424\n",
       "|    └─Sequential: 2-7                   [-1, 256, 14, 14]         --\n",
       "|    |    └─BasicBlock: 3-5              [-1, 256, 14, 14]         919,040\n",
       "|    |    └─BasicBlock: 3-6              [-1, 256, 14, 14]         1,180,672\n",
       "|    └─Sequential: 2-8                   [-1, 512, 7, 7]           --\n",
       "|    |    └─BasicBlock: 3-7              [-1, 512, 7, 7]           3,673,088\n",
       "|    |    └─BasicBlock: 3-8              [-1, 512, 7, 7]           4,720,640\n",
       "|    └─AdaptiveAvgPool2d: 2-9            [-1, 512, 1, 1]           --\n",
       "|    └─Linear: 2-10                      [-1, 2000]                1,026,000\n",
       "==========================================================================================\n",
       "Total params: 12,202,512\n",
       "Trainable params: 12,202,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.83\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 35.23\n",
       "Params size (MB): 46.55\n",
       "Estimated Total Size (MB): 82.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary; adjust the input shape as needed (here, (3, 224, 224) for an RGB image)\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(model.resnet.conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1) Create the ./models/ directory if not exists\n",
    "# -------------------------------------------------\n",
    "os.makedirs(\"./models_2k\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 5) Training Parameters\n",
    "# -------------------------------------------------\n",
    "num_iterations_per_epoch = 1000  # As per paper's definition of \"epoch\"\n",
    "max_epochs = MAX_EPOCHS          # Upper bound on epochs == 500 # Approximately 5 days with 14.5 mins per epoch on RTX 3090\n",
    "patience = 30                    # Early stopping patience (in “paper-defined epochs”)\n",
    "checkpoint_interval = 5\n",
    "\n",
    "best_val_f1 = float('-inf')     # Track best validation F1 so far\n",
    "stagnant_epochs = 0             # Count epochs without improvement\n",
    "start_epoch = 0                 # If resuming, we will update this\n",
    "\n",
    "checkpoint_path = \"./models_2k/checkpoint_last_2k.pth\"\n",
    "best_model_state_dict_path = \"./models_2k/resnet18_pretrained_2k.pth\"\n",
    "best_model_full_path = \"./models_2k/resnet18_pretrained_full_2k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 225 with best_val_f1=0.7782\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 6) If Checkpoint Exists, Resume\n",
    "# -------------------------------------------------\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    best_val_f1 = checkpoint[\"best_val_f1\"]\n",
    "    stagnant_epochs = checkpoint[\"stagnant_epochs\"]\n",
    "    start_epoch = checkpoint[\"epoch\"]  # We stored epoch in the checkpoint\n",
    "    print(f\"Resuming training from epoch {start_epoch} with best_val_f1={best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226], Loss: 0.417255, LR: 1.988520e-05, Train F1: 0.7852, Train Acc: 0.8059, Val F1: 0.7753, Val Acc: 0.7963\n",
      "LR updated from 1.988520e-05 to 1.988469e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [227], Loss: 0.415667, LR: 1.988469e-05, Train F1: 0.7885, Train Acc: 0.8099, Val F1: 0.7797, Val Acc: 0.8013\n",
      "LR updated from 1.988469e-05 to 1.988418e-05\n",
      "** New best model found! Val F1 improved to 0.7797. Model saved.\n",
      "Epoch [228], Loss: 0.416571, LR: 1.988418e-05, Train F1: 0.7804, Train Acc: 0.8034, Val F1: 0.7713, Val Acc: 0.7947\n",
      "LR updated from 1.988418e-05 to 1.988368e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [229], Loss: 0.401082, LR: 1.988368e-05, Train F1: 0.7868, Train Acc: 0.8095, Val F1: 0.7782, Val Acc: 0.8010\n",
      "LR updated from 1.988368e-05 to 1.988317e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [230], Loss: 0.407652, LR: 1.988317e-05, Train F1: 0.7883, Train Acc: 0.8099, Val F1: 0.7794, Val Acc: 0.8011\n",
      "LR updated from 1.988317e-05 to 1.988266e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [231], Loss: 0.409726, LR: 1.988266e-05, Train F1: 0.7828, Train Acc: 0.8063, Val F1: 0.7732, Val Acc: 0.7966\n",
      "LR updated from 1.988266e-05 to 1.988215e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [232], Loss: 0.416079, LR: 1.988215e-05, Train F1: 0.7841, Train Acc: 0.8057, Val F1: 0.7752, Val Acc: 0.7970\n",
      "LR updated from 1.988215e-05 to 1.988164e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [233], Loss: 0.410131, LR: 1.988164e-05, Train F1: 0.7852, Train Acc: 0.8081, Val F1: 0.7761, Val Acc: 0.7988\n",
      "LR updated from 1.988164e-05 to 1.988113e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [234], Loss: 0.408571, LR: 1.988113e-05, Train F1: 0.7898, Train Acc: 0.8111, Val F1: 0.7802, Val Acc: 0.8012\n",
      "LR updated from 1.988113e-05 to 1.988062e-05\n",
      "** New best model found! Val F1 improved to 0.7802. Model saved.\n",
      "Epoch [235], Loss: 0.407662, LR: 1.988062e-05, Train F1: 0.7846, Train Acc: 0.8071, Val F1: 0.7760, Val Acc: 0.7987\n",
      "LR updated from 1.988062e-05 to 1.988011e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [236], Loss: 0.406593, LR: 1.988011e-05, Train F1: 0.7859, Train Acc: 0.8087, Val F1: 0.7769, Val Acc: 0.7998\n",
      "LR updated from 1.988011e-05 to 1.987961e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [237], Loss: 0.401516, LR: 1.987961e-05, Train F1: 0.7850, Train Acc: 0.8089, Val F1: 0.7755, Val Acc: 0.7992\n",
      "LR updated from 1.987961e-05 to 1.987910e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [238], Loss: 0.405489, LR: 1.987910e-05, Train F1: 0.7862, Train Acc: 0.8089, Val F1: 0.7768, Val Acc: 0.7997\n",
      "LR updated from 1.987910e-05 to 1.987859e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [239], Loss: 0.408434, LR: 1.987859e-05, Train F1: 0.7873, Train Acc: 0.8088, Val F1: 0.7778, Val Acc: 0.7997\n",
      "LR updated from 1.987859e-05 to 1.987808e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [240], Loss: 0.403422, LR: 1.987808e-05, Train F1: 0.7882, Train Acc: 0.8101, Val F1: 0.7783, Val Acc: 0.8006\n",
      "LR updated from 1.987808e-05 to 1.987757e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [241], Loss: 0.408726, LR: 1.987757e-05, Train F1: 0.7859, Train Acc: 0.8088, Val F1: 0.7766, Val Acc: 0.7995\n",
      "LR updated from 1.987757e-05 to 1.987706e-05\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [242], Loss: 0.401691, LR: 1.987706e-05, Train F1: 0.7905, Train Acc: 0.8116, Val F1: 0.7814, Val Acc: 0.8028\n",
      "LR updated from 1.987706e-05 to 1.987655e-05\n",
      "** New best model found! Val F1 improved to 0.7814. Model saved.\n",
      "Epoch [243], Loss: 0.400640, LR: 1.987655e-05, Train F1: 0.7882, Train Acc: 0.8098, Val F1: 0.7795, Val Acc: 0.8015\n",
      "LR updated from 1.987655e-05 to 1.987605e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [244], Loss: 0.400311, LR: 1.987605e-05, Train F1: 0.7873, Train Acc: 0.8094, Val F1: 0.7785, Val Acc: 0.8006\n",
      "LR updated from 1.987605e-05 to 1.987554e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [245], Loss: 0.400990, LR: 1.987554e-05, Train F1: 0.7869, Train Acc: 0.8100, Val F1: 0.7784, Val Acc: 0.8016\n",
      "LR updated from 1.987554e-05 to 1.987503e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [246], Loss: 0.409744, LR: 1.987503e-05, Train F1: 0.7822, Train Acc: 0.8059, Val F1: 0.7732, Val Acc: 0.7973\n",
      "LR updated from 1.987503e-05 to 1.987452e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [247], Loss: 0.402378, LR: 1.987452e-05, Train F1: 0.7916, Train Acc: 0.8139, Val F1: 0.7823, Val Acc: 0.8045\n",
      "LR updated from 1.987452e-05 to 1.987401e-05\n",
      "** New best model found! Val F1 improved to 0.7823. Model saved.\n",
      "Epoch [248], Loss: 0.397362, LR: 1.987401e-05, Train F1: 0.7896, Train Acc: 0.8124, Val F1: 0.7798, Val Acc: 0.8024\n",
      "LR updated from 1.987401e-05 to 1.987350e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [249], Loss: 0.401235, LR: 1.987350e-05, Train F1: 0.7866, Train Acc: 0.8103, Val F1: 0.7774, Val Acc: 0.8011\n",
      "LR updated from 1.987350e-05 to 1.987300e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [250], Loss: 0.396709, LR: 1.987300e-05, Train F1: 0.7894, Train Acc: 0.8127, Val F1: 0.7805, Val Acc: 0.8036\n",
      "LR updated from 1.987300e-05 to 1.987249e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [251], Loss: 0.398952, LR: 1.987249e-05, Train F1: 0.7933, Train Acc: 0.8149, Val F1: 0.7834, Val Acc: 0.8050\n",
      "LR updated from 1.987249e-05 to 1.987198e-05\n",
      "** New best model found! Val F1 improved to 0.7834. Model saved.\n",
      "Epoch [252], Loss: 0.400568, LR: 1.987198e-05, Train F1: 0.7910, Train Acc: 0.8129, Val F1: 0.7819, Val Acc: 0.8040\n",
      "LR updated from 1.987198e-05 to 1.987147e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [253], Loss: 0.396436, LR: 1.987147e-05, Train F1: 0.7891, Train Acc: 0.8117, Val F1: 0.7795, Val Acc: 0.8022\n",
      "LR updated from 1.987147e-05 to 1.987096e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [254], Loss: 0.392710, LR: 1.987096e-05, Train F1: 0.7902, Train Acc: 0.8134, Val F1: 0.7811, Val Acc: 0.8045\n",
      "LR updated from 1.987096e-05 to 1.987045e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [255], Loss: 0.393502, LR: 1.987045e-05, Train F1: 0.7918, Train Acc: 0.8137, Val F1: 0.7819, Val Acc: 0.8041\n",
      "LR updated from 1.987045e-05 to 1.986994e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [256], Loss: 0.393967, LR: 1.986994e-05, Train F1: 0.7919, Train Acc: 0.8142, Val F1: 0.7825, Val Acc: 0.8049\n",
      "LR updated from 1.986994e-05 to 1.986944e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [257], Loss: 0.393466, LR: 1.986944e-05, Train F1: 0.7910, Train Acc: 0.8128, Val F1: 0.7809, Val Acc: 0.8026\n",
      "LR updated from 1.986944e-05 to 1.986893e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [258], Loss: 0.393532, LR: 1.986893e-05, Train F1: 0.7946, Train Acc: 0.8154, Val F1: 0.7850, Val Acc: 0.8061\n",
      "LR updated from 1.986893e-05 to 1.986842e-05\n",
      "** New best model found! Val F1 improved to 0.7850. Model saved.\n",
      "Epoch [259], Loss: 0.390154, LR: 1.986842e-05, Train F1: 0.7916, Train Acc: 0.8137, Val F1: 0.7819, Val Acc: 0.8046\n",
      "LR updated from 1.986842e-05 to 1.986791e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [260], Loss: 0.388729, LR: 1.986791e-05, Train F1: 0.7925, Train Acc: 0.8155, Val F1: 0.7829, Val Acc: 0.8064\n",
      "LR updated from 1.986791e-05 to 1.986740e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [261], Loss: 0.391450, LR: 1.986740e-05, Train F1: 0.7934, Train Acc: 0.8157, Val F1: 0.7839, Val Acc: 0.8066\n",
      "LR updated from 1.986740e-05 to 1.986689e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [262], Loss: 0.384162, LR: 1.986689e-05, Train F1: 0.7929, Train Acc: 0.8150, Val F1: 0.7832, Val Acc: 0.8055\n",
      "LR updated from 1.986689e-05 to 1.986639e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [263], Loss: 0.389099, LR: 1.986639e-05, Train F1: 0.7934, Train Acc: 0.8147, Val F1: 0.7836, Val Acc: 0.8050\n",
      "LR updated from 1.986639e-05 to 1.986588e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [264], Loss: 0.392037, LR: 1.986588e-05, Train F1: 0.7870, Train Acc: 0.8095, Val F1: 0.7775, Val Acc: 0.8001\n",
      "LR updated from 1.986588e-05 to 1.986537e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [265], Loss: 0.379559, LR: 1.986537e-05, Train F1: 0.7936, Train Acc: 0.8165, Val F1: 0.7842, Val Acc: 0.8074\n",
      "LR updated from 1.986537e-05 to 1.986486e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [266], Loss: 0.387506, LR: 1.986486e-05, Train F1: 0.7908, Train Acc: 0.8121, Val F1: 0.7812, Val Acc: 0.8027\n",
      "LR updated from 1.986486e-05 to 1.986435e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [267], Loss: 0.388275, LR: 1.986435e-05, Train F1: 0.7941, Train Acc: 0.8165, Val F1: 0.7840, Val Acc: 0.8063\n",
      "LR updated from 1.986435e-05 to 1.986385e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [268], Loss: 0.384112, LR: 1.986385e-05, Train F1: 0.7910, Train Acc: 0.8140, Val F1: 0.7816, Val Acc: 0.8047\n",
      "LR updated from 1.986385e-05 to 1.986334e-05\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [269], Loss: 0.388472, LR: 1.986334e-05, Train F1: 0.7919, Train Acc: 0.8145, Val F1: 0.7816, Val Acc: 0.8044\n",
      "LR updated from 1.986334e-05 to 1.986283e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [270], Loss: 0.375685, LR: 1.986283e-05, Train F1: 0.7924, Train Acc: 0.8151, Val F1: 0.7825, Val Acc: 0.8056\n",
      "LR updated from 1.986283e-05 to 1.986232e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 12 epoch(s).\n",
      "Epoch [271], Loss: 0.385823, LR: 1.986232e-05, Train F1: 0.7887, Train Acc: 0.8122, Val F1: 0.7796, Val Acc: 0.8032\n",
      "LR updated from 1.986232e-05 to 1.986181e-05\n",
      "No improvement in Val F1 for 13 epoch(s).\n",
      "Epoch [272], Loss: 0.384543, LR: 1.986181e-05, Train F1: 0.7928, Train Acc: 0.8159, Val F1: 0.7827, Val Acc: 0.8059\n",
      "LR updated from 1.986181e-05 to 1.986130e-05\n",
      "No improvement in Val F1 for 14 epoch(s).\n",
      "Epoch [273], Loss: 0.387943, LR: 1.986130e-05, Train F1: 0.7925, Train Acc: 0.8154, Val F1: 0.7827, Val Acc: 0.8058\n",
      "LR updated from 1.986130e-05 to 1.986080e-05\n",
      "No improvement in Val F1 for 15 epoch(s).\n",
      "Epoch [274], Loss: 0.379892, LR: 1.986080e-05, Train F1: 0.7944, Train Acc: 0.8164, Val F1: 0.7850, Val Acc: 0.8070\n",
      "LR updated from 1.986080e-05 to 1.986029e-05\n",
      "No improvement in Val F1 for 16 epoch(s).\n",
      "Epoch [275], Loss: 0.379782, LR: 1.986029e-05, Train F1: 0.7955, Train Acc: 0.8167, Val F1: 0.7853, Val Acc: 0.8064\n",
      "LR updated from 1.986029e-05 to 1.985978e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "** New best model found! Val F1 improved to 0.7853. Model saved.\n",
      "Epoch [276], Loss: 0.379575, LR: 1.985978e-05, Train F1: 0.7934, Train Acc: 0.8152, Val F1: 0.7843, Val Acc: 0.8062\n",
      "LR updated from 1.985978e-05 to 1.985927e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [277], Loss: 0.383629, LR: 1.985927e-05, Train F1: 0.7953, Train Acc: 0.8176, Val F1: 0.7857, Val Acc: 0.8081\n",
      "LR updated from 1.985927e-05 to 1.985876e-05\n",
      "** New best model found! Val F1 improved to 0.7857. Model saved.\n",
      "Epoch [278], Loss: 0.384132, LR: 1.985876e-05, Train F1: 0.7899, Train Acc: 0.8124, Val F1: 0.7804, Val Acc: 0.8032\n",
      "LR updated from 1.985876e-05 to 1.985826e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [279], Loss: 0.383313, LR: 1.985826e-05, Train F1: 0.7961, Train Acc: 0.8181, Val F1: 0.7861, Val Acc: 0.8081\n",
      "LR updated from 1.985826e-05 to 1.985775e-05\n",
      "** New best model found! Val F1 improved to 0.7861. Model saved.\n",
      "Epoch [280], Loss: 0.373405, LR: 1.985775e-05, Train F1: 0.7958, Train Acc: 0.8168, Val F1: 0.7863, Val Acc: 0.8076\n",
      "LR updated from 1.985775e-05 to 1.985724e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "** New best model found! Val F1 improved to 0.7863. Model saved.\n",
      "Epoch [281], Loss: 0.381807, LR: 1.985724e-05, Train F1: 0.7978, Train Acc: 0.8181, Val F1: 0.7873, Val Acc: 0.8080\n",
      "LR updated from 1.985724e-05 to 1.985673e-05\n",
      "** New best model found! Val F1 improved to 0.7873. Model saved.\n",
      "Epoch [282], Loss: 0.381472, LR: 1.985673e-05, Train F1: 0.7932, Train Acc: 0.8157, Val F1: 0.7832, Val Acc: 0.8058\n",
      "LR updated from 1.985673e-05 to 1.985622e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [283], Loss: 0.382785, LR: 1.985622e-05, Train F1: 0.7951, Train Acc: 0.8163, Val F1: 0.7853, Val Acc: 0.8067\n",
      "LR updated from 1.985622e-05 to 1.985572e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [284], Loss: 0.381134, LR: 1.985572e-05, Train F1: 0.7966, Train Acc: 0.8185, Val F1: 0.7865, Val Acc: 0.8085\n",
      "LR updated from 1.985572e-05 to 1.985521e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [285], Loss: 0.376216, LR: 1.985521e-05, Train F1: 0.7974, Train Acc: 0.8186, Val F1: 0.7870, Val Acc: 0.8084\n",
      "LR updated from 1.985521e-05 to 1.985470e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [286], Loss: 0.373666, LR: 1.985470e-05, Train F1: 0.7967, Train Acc: 0.8190, Val F1: 0.7861, Val Acc: 0.8088\n",
      "LR updated from 1.985470e-05 to 1.985419e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [287], Loss: 0.381622, LR: 1.985419e-05, Train F1: 0.7966, Train Acc: 0.8180, Val F1: 0.7864, Val Acc: 0.8080\n",
      "LR updated from 1.985419e-05 to 1.985368e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [288], Loss: 0.370597, LR: 1.985368e-05, Train F1: 0.8000, Train Acc: 0.8200, Val F1: 0.7896, Val Acc: 0.8099\n",
      "LR updated from 1.985368e-05 to 1.985318e-05\n",
      "** New best model found! Val F1 improved to 0.7896. Model saved.\n",
      "Epoch [289], Loss: 0.370206, LR: 1.985318e-05, Train F1: 0.7985, Train Acc: 0.8209, Val F1: 0.7884, Val Acc: 0.8107\n",
      "LR updated from 1.985318e-05 to 1.985267e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [290], Loss: 0.379939, LR: 1.985267e-05, Train F1: 0.7959, Train Acc: 0.8186, Val F1: 0.7857, Val Acc: 0.8086\n",
      "LR updated from 1.985267e-05 to 1.985216e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [291], Loss: 0.370478, LR: 1.985216e-05, Train F1: 0.7964, Train Acc: 0.8190, Val F1: 0.7864, Val Acc: 0.8094\n",
      "LR updated from 1.985216e-05 to 1.985165e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [292], Loss: 0.380465, LR: 1.985165e-05, Train F1: 0.7987, Train Acc: 0.8203, Val F1: 0.7885, Val Acc: 0.8101\n",
      "LR updated from 1.985165e-05 to 1.985114e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [293], Loss: 0.372414, LR: 1.985114e-05, Train F1: 0.7968, Train Acc: 0.8184, Val F1: 0.7866, Val Acc: 0.8085\n",
      "LR updated from 1.985114e-05 to 1.985064e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [294], Loss: 0.365852, LR: 1.985064e-05, Train F1: 0.7989, Train Acc: 0.8205, Val F1: 0.7884, Val Acc: 0.8099\n",
      "LR updated from 1.985064e-05 to 1.985013e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [295], Loss: 0.372680, LR: 1.985013e-05, Train F1: 0.7967, Train Acc: 0.8187, Val F1: 0.7865, Val Acc: 0.8088\n",
      "LR updated from 1.985013e-05 to 1.984962e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [296], Loss: 0.368256, LR: 1.984962e-05, Train F1: 0.7982, Train Acc: 0.8187, Val F1: 0.7871, Val Acc: 0.8081\n",
      "LR updated from 1.984962e-05 to 1.984911e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [297], Loss: 0.367296, LR: 1.984911e-05, Train F1: 0.7968, Train Acc: 0.8176, Val F1: 0.7862, Val Acc: 0.8074\n",
      "LR updated from 1.984911e-05 to 1.984861e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [298], Loss: 0.371628, LR: 1.984861e-05, Train F1: 0.7987, Train Acc: 0.8196, Val F1: 0.7878, Val Acc: 0.8091\n",
      "LR updated from 1.984861e-05 to 1.984810e-05\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [299], Loss: 0.377603, LR: 1.984810e-05, Train F1: 0.7963, Train Acc: 0.8169, Val F1: 0.7861, Val Acc: 0.8069\n",
      "LR updated from 1.984810e-05 to 1.984759e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [300], Loss: 0.365207, LR: 1.984759e-05, Train F1: 0.8017, Train Acc: 0.8210, Val F1: 0.7900, Val Acc: 0.8094\n",
      "LR updated from 1.984759e-05 to 1.984708e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "** New best model found! Val F1 improved to 0.7900. Model saved.\n",
      "Epoch [301], Loss: 0.369106, LR: 1.984708e-05, Train F1: 0.7999, Train Acc: 0.8200, Val F1: 0.7883, Val Acc: 0.8085\n",
      "LR updated from 1.984708e-05 to 1.984657e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [302], Loss: 0.370186, LR: 1.984657e-05, Train F1: 0.7999, Train Acc: 0.8209, Val F1: 0.7897, Val Acc: 0.8110\n",
      "LR updated from 1.984657e-05 to 1.984607e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [303], Loss: 0.370259, LR: 1.984607e-05, Train F1: 0.8000, Train Acc: 0.8196, Val F1: 0.7883, Val Acc: 0.8079\n",
      "LR updated from 1.984607e-05 to 1.984556e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [304], Loss: 0.366738, LR: 1.984556e-05, Train F1: 0.7969, Train Acc: 0.8189, Val F1: 0.7864, Val Acc: 0.8087\n",
      "LR updated from 1.984556e-05 to 1.984505e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [305], Loss: 0.366584, LR: 1.984505e-05, Train F1: 0.7989, Train Acc: 0.8201, Val F1: 0.7884, Val Acc: 0.8101\n",
      "LR updated from 1.984505e-05 to 1.984454e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [306], Loss: 0.372653, LR: 1.984454e-05, Train F1: 0.8011, Train Acc: 0.8210, Val F1: 0.7898, Val Acc: 0.8099\n",
      "LR updated from 1.984454e-05 to 1.984404e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [307], Loss: 0.363181, LR: 1.984404e-05, Train F1: 0.7954, Train Acc: 0.8179, Val F1: 0.7847, Val Acc: 0.8072\n",
      "LR updated from 1.984404e-05 to 1.984353e-05\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [308], Loss: 0.364127, LR: 1.984353e-05, Train F1: 0.7982, Train Acc: 0.8198, Val F1: 0.7877, Val Acc: 0.8096\n",
      "LR updated from 1.984353e-05 to 1.984302e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [309], Loss: 0.367335, LR: 1.984302e-05, Train F1: 0.7993, Train Acc: 0.8207, Val F1: 0.7887, Val Acc: 0.8108\n",
      "LR updated from 1.984302e-05 to 1.984251e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [310], Loss: 0.365178, LR: 1.984251e-05, Train F1: 0.8003, Train Acc: 0.8213, Val F1: 0.7886, Val Acc: 0.8103\n",
      "LR updated from 1.984251e-05 to 1.984200e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [311], Loss: 0.364329, LR: 1.984200e-05, Train F1: 0.7999, Train Acc: 0.8223, Val F1: 0.7898, Val Acc: 0.8125\n",
      "LR updated from 1.984200e-05 to 1.984150e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [312], Loss: 0.365677, LR: 1.984150e-05, Train F1: 0.8012, Train Acc: 0.8212, Val F1: 0.7899, Val Acc: 0.8104\n",
      "LR updated from 1.984150e-05 to 1.984099e-05\n",
      "No improvement in Val F1 for 12 epoch(s).\n",
      "Epoch [313], Loss: 0.364730, LR: 1.984099e-05, Train F1: 0.8002, Train Acc: 0.8219, Val F1: 0.7892, Val Acc: 0.8111\n",
      "LR updated from 1.984099e-05 to 1.984048e-05\n",
      "No improvement in Val F1 for 13 epoch(s).\n",
      "Epoch [314], Loss: 0.362835, LR: 1.984048e-05, Train F1: 0.8003, Train Acc: 0.8224, Val F1: 0.7898, Val Acc: 0.8119\n",
      "LR updated from 1.984048e-05 to 1.983997e-05\n",
      "No improvement in Val F1 for 14 epoch(s).\n",
      "Epoch [315], Loss: 0.362962, LR: 1.983997e-05, Train F1: 0.8001, Train Acc: 0.8220, Val F1: 0.7889, Val Acc: 0.8106\n",
      "LR updated from 1.983997e-05 to 1.983947e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 15 epoch(s).\n",
      "Epoch [316], Loss: 0.361605, LR: 1.983947e-05, Train F1: 0.8020, Train Acc: 0.8230, Val F1: 0.7908, Val Acc: 0.8121\n",
      "LR updated from 1.983947e-05 to 1.983896e-05\n",
      "** New best model found! Val F1 improved to 0.7908. Model saved.\n",
      "Epoch [317], Loss: 0.369075, LR: 1.983896e-05, Train F1: 0.7995, Train Acc: 0.8217, Val F1: 0.7885, Val Acc: 0.8110\n",
      "LR updated from 1.983896e-05 to 1.983845e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [318], Loss: 0.361528, LR: 1.983845e-05, Train F1: 0.8020, Train Acc: 0.8224, Val F1: 0.7906, Val Acc: 0.8115\n",
      "LR updated from 1.983845e-05 to 1.983794e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [319], Loss: 0.365511, LR: 1.983794e-05, Train F1: 0.8035, Train Acc: 0.8241, Val F1: 0.7919, Val Acc: 0.8123\n",
      "LR updated from 1.983794e-05 to 1.983744e-05\n",
      "** New best model found! Val F1 improved to 0.7919. Model saved.\n",
      "Epoch [320], Loss: 0.360546, LR: 1.983744e-05, Train F1: 0.8008, Train Acc: 0.8230, Val F1: 0.7899, Val Acc: 0.8125\n",
      "LR updated from 1.983744e-05 to 1.983693e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [321], Loss: 0.361239, LR: 1.983693e-05, Train F1: 0.8027, Train Acc: 0.8244, Val F1: 0.7917, Val Acc: 0.8137\n",
      "LR updated from 1.983693e-05 to 1.983642e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [322], Loss: 0.363260, LR: 1.983642e-05, Train F1: 0.8004, Train Acc: 0.8220, Val F1: 0.7896, Val Acc: 0.8115\n",
      "LR updated from 1.983642e-05 to 1.983591e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [323], Loss: 0.361998, LR: 1.983591e-05, Train F1: 0.8011, Train Acc: 0.8227, Val F1: 0.7902, Val Acc: 0.8123\n",
      "LR updated from 1.983591e-05 to 1.983541e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [324], Loss: 0.359157, LR: 1.983541e-05, Train F1: 0.7987, Train Acc: 0.8207, Val F1: 0.7883, Val Acc: 0.8108\n",
      "LR updated from 1.983541e-05 to 1.983490e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [325], Loss: 0.353170, LR: 1.983490e-05, Train F1: 0.8022, Train Acc: 0.8222, Val F1: 0.7918, Val Acc: 0.8123\n",
      "LR updated from 1.983490e-05 to 1.983439e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [326], Loss: 0.359383, LR: 1.983439e-05, Train F1: 0.8036, Train Acc: 0.8239, Val F1: 0.7919, Val Acc: 0.8123\n",
      "LR updated from 1.983439e-05 to 1.983388e-05\n",
      "** New best model found! Val F1 improved to 0.7919. Model saved.\n",
      "Epoch [327], Loss: 0.357943, LR: 1.983388e-05, Train F1: 0.8013, Train Acc: 0.8239, Val F1: 0.7903, Val Acc: 0.8127\n",
      "LR updated from 1.983388e-05 to 1.983338e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [328], Loss: 0.357276, LR: 1.983338e-05, Train F1: 0.8044, Train Acc: 0.8248, Val F1: 0.7937, Val Acc: 0.8143\n",
      "LR updated from 1.983338e-05 to 1.983287e-05\n",
      "** New best model found! Val F1 improved to 0.7937. Model saved.\n",
      "Epoch [329], Loss: 0.359175, LR: 1.983287e-05, Train F1: 0.8023, Train Acc: 0.8231, Val F1: 0.7915, Val Acc: 0.8124\n",
      "LR updated from 1.983287e-05 to 1.983236e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [330], Loss: 0.352599, LR: 1.983236e-05, Train F1: 0.8038, Train Acc: 0.8239, Val F1: 0.7925, Val Acc: 0.8127\n",
      "LR updated from 1.983236e-05 to 1.983185e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [331], Loss: 0.361712, LR: 1.983185e-05, Train F1: 0.8051, Train Acc: 0.8256, Val F1: 0.7933, Val Acc: 0.8136\n",
      "LR updated from 1.983185e-05 to 1.983135e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [332], Loss: 0.357920, LR: 1.983135e-05, Train F1: 0.8014, Train Acc: 0.8213, Val F1: 0.7906, Val Acc: 0.8108\n",
      "LR updated from 1.983135e-05 to 1.983084e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [333], Loss: 0.352182, LR: 1.983084e-05, Train F1: 0.8001, Train Acc: 0.8219, Val F1: 0.7894, Val Acc: 0.8113\n",
      "LR updated from 1.983084e-05 to 1.983033e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [334], Loss: 0.355059, LR: 1.983033e-05, Train F1: 0.8010, Train Acc: 0.8233, Val F1: 0.7905, Val Acc: 0.8128\n",
      "LR updated from 1.983033e-05 to 1.982983e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [335], Loss: 0.356434, LR: 1.982983e-05, Train F1: 0.8049, Train Acc: 0.8249, Val F1: 0.7930, Val Acc: 0.8132\n",
      "LR updated from 1.982983e-05 to 1.982932e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [336], Loss: 0.352303, LR: 1.982932e-05, Train F1: 0.8019, Train Acc: 0.8237, Val F1: 0.7907, Val Acc: 0.8126\n",
      "LR updated from 1.982932e-05 to 1.982881e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [337], Loss: 0.355746, LR: 1.982881e-05, Train F1: 0.8004, Train Acc: 0.8221, Val F1: 0.7896, Val Acc: 0.8114\n",
      "LR updated from 1.982881e-05 to 1.982830e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [338], Loss: 0.355287, LR: 1.982830e-05, Train F1: 0.8033, Train Acc: 0.8244, Val F1: 0.7917, Val Acc: 0.8131\n",
      "LR updated from 1.982830e-05 to 1.982780e-05\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [339], Loss: 0.353736, LR: 1.982780e-05, Train F1: 0.8023, Train Acc: 0.8237, Val F1: 0.7909, Val Acc: 0.8126\n",
      "LR updated from 1.982780e-05 to 1.982729e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [340], Loss: 0.347483, LR: 1.982729e-05, Train F1: 0.8037, Train Acc: 0.8244, Val F1: 0.7918, Val Acc: 0.8130\n",
      "LR updated from 1.982729e-05 to 1.982678e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 12 epoch(s).\n",
      "Epoch [341], Loss: 0.351969, LR: 1.982678e-05, Train F1: 0.8036, Train Acc: 0.8247, Val F1: 0.7916, Val Acc: 0.8128\n",
      "LR updated from 1.982678e-05 to 1.982627e-05\n",
      "No improvement in Val F1 for 13 epoch(s).\n",
      "Epoch [342], Loss: 0.352976, LR: 1.982627e-05, Train F1: 0.8009, Train Acc: 0.8222, Val F1: 0.7899, Val Acc: 0.8114\n",
      "LR updated from 1.982627e-05 to 1.982577e-05\n",
      "No improvement in Val F1 for 14 epoch(s).\n",
      "Epoch [343], Loss: 0.355066, LR: 1.982577e-05, Train F1: 0.8047, Train Acc: 0.8247, Val F1: 0.7925, Val Acc: 0.8128\n",
      "LR updated from 1.982577e-05 to 1.982526e-05\n",
      "No improvement in Val F1 for 15 epoch(s).\n",
      "Epoch [344], Loss: 0.353668, LR: 1.982526e-05, Train F1: 0.8040, Train Acc: 0.8255, Val F1: 0.7924, Val Acc: 0.8142\n",
      "LR updated from 1.982526e-05 to 1.982475e-05\n",
      "No improvement in Val F1 for 16 epoch(s).\n",
      "Epoch [345], Loss: 0.353551, LR: 1.982475e-05, Train F1: 0.8034, Train Acc: 0.8253, Val F1: 0.7923, Val Acc: 0.8147\n",
      "LR updated from 1.982475e-05 to 1.982425e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 17 epoch(s).\n",
      "Epoch [346], Loss: 0.351324, LR: 1.982425e-05, Train F1: 0.8065, Train Acc: 0.8261, Val F1: 0.7945, Val Acc: 0.8143\n",
      "LR updated from 1.982425e-05 to 1.982374e-05\n",
      "** New best model found! Val F1 improved to 0.7945. Model saved.\n",
      "Epoch [347], Loss: 0.348366, LR: 1.982374e-05, Train F1: 0.8027, Train Acc: 0.8250, Val F1: 0.7914, Val Acc: 0.8139\n",
      "LR updated from 1.982374e-05 to 1.982323e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [348], Loss: 0.355119, LR: 1.982323e-05, Train F1: 0.8037, Train Acc: 0.8240, Val F1: 0.7914, Val Acc: 0.8121\n",
      "LR updated from 1.982323e-05 to 1.982272e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [349], Loss: 0.347223, LR: 1.982272e-05, Train F1: 0.8062, Train Acc: 0.8264, Val F1: 0.7944, Val Acc: 0.8151\n",
      "LR updated from 1.982272e-05 to 1.982222e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [350], Loss: 0.355648, LR: 1.982222e-05, Train F1: 0.8046, Train Acc: 0.8259, Val F1: 0.7927, Val Acc: 0.8140\n",
      "LR updated from 1.982222e-05 to 1.982171e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [351], Loss: 0.347228, LR: 1.982171e-05, Train F1: 0.8028, Train Acc: 0.8250, Val F1: 0.7914, Val Acc: 0.8138\n",
      "LR updated from 1.982171e-05 to 1.982120e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [352], Loss: 0.349953, LR: 1.982120e-05, Train F1: 0.8045, Train Acc: 0.8268, Val F1: 0.7932, Val Acc: 0.8156\n",
      "LR updated from 1.982120e-05 to 1.982070e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [353], Loss: 0.346788, LR: 1.982070e-05, Train F1: 0.8037, Train Acc: 0.8253, Val F1: 0.7929, Val Acc: 0.8148\n",
      "LR updated from 1.982070e-05 to 1.982019e-05\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [354], Loss: 0.349940, LR: 1.982019e-05, Train F1: 0.8040, Train Acc: 0.8248, Val F1: 0.7922, Val Acc: 0.8133\n",
      "LR updated from 1.982019e-05 to 1.981968e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [355], Loss: 0.346961, LR: 1.981968e-05, Train F1: 0.8055, Train Acc: 0.8273, Val F1: 0.7944, Val Acc: 0.8167\n",
      "LR updated from 1.981968e-05 to 1.981917e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [356], Loss: 0.348859, LR: 1.981917e-05, Train F1: 0.8022, Train Acc: 0.8230, Val F1: 0.7906, Val Acc: 0.8116\n",
      "LR updated from 1.981917e-05 to 1.981867e-05\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [357], Loss: 0.351164, LR: 1.981867e-05, Train F1: 0.8041, Train Acc: 0.8250, Val F1: 0.7929, Val Acc: 0.8140\n",
      "LR updated from 1.981867e-05 to 1.981816e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [358], Loss: 0.344877, LR: 1.981816e-05, Train F1: 0.8081, Train Acc: 0.8276, Val F1: 0.7958, Val Acc: 0.8154\n",
      "LR updated from 1.981816e-05 to 1.981765e-05\n",
      "** New best model found! Val F1 improved to 0.7958. Model saved.\n",
      "Epoch [359], Loss: 0.347958, LR: 1.981765e-05, Train F1: 0.8051, Train Acc: 0.8255, Val F1: 0.7930, Val Acc: 0.8133\n",
      "LR updated from 1.981765e-05 to 1.981715e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [360], Loss: 0.349246, LR: 1.981715e-05, Train F1: 0.8061, Train Acc: 0.8275, Val F1: 0.7940, Val Acc: 0.8159\n",
      "LR updated from 1.981715e-05 to 1.981664e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [361], Loss: 0.338637, LR: 1.981664e-05, Train F1: 0.8040, Train Acc: 0.8255, Val F1: 0.7920, Val Acc: 0.8138\n",
      "LR updated from 1.981664e-05 to 1.981613e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [362], Loss: 0.345251, LR: 1.981613e-05, Train F1: 0.8037, Train Acc: 0.8240, Val F1: 0.7921, Val Acc: 0.8126\n",
      "LR updated from 1.981613e-05 to 1.981562e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [363], Loss: 0.345180, LR: 1.981562e-05, Train F1: 0.8052, Train Acc: 0.8273, Val F1: 0.7940, Val Acc: 0.8165\n",
      "LR updated from 1.981562e-05 to 1.981512e-05\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [364], Loss: 0.346226, LR: 1.981512e-05, Train F1: 0.8039, Train Acc: 0.8262, Val F1: 0.7929, Val Acc: 0.8155\n",
      "LR updated from 1.981512e-05 to 1.981461e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [365], Loss: 0.346970, LR: 1.981461e-05, Train F1: 0.8077, Train Acc: 0.8276, Val F1: 0.7957, Val Acc: 0.8159\n",
      "LR updated from 1.981461e-05 to 1.981410e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [366], Loss: 0.344095, LR: 1.981410e-05, Train F1: 0.8005, Train Acc: 0.8208, Val F1: 0.7890, Val Acc: 0.8095\n",
      "LR updated from 1.981410e-05 to 1.981360e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [367], Loss: 0.348324, LR: 1.981360e-05, Train F1: 0.8034, Train Acc: 0.8258, Val F1: 0.7918, Val Acc: 0.8146\n",
      "LR updated from 1.981360e-05 to 1.981309e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [368], Loss: 0.338838, LR: 1.981309e-05, Train F1: 0.8051, Train Acc: 0.8273, Val F1: 0.7938, Val Acc: 0.8165\n",
      "LR updated from 1.981309e-05 to 1.981258e-05\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [369], Loss: 0.342575, LR: 1.981258e-05, Train F1: 0.8066, Train Acc: 0.8268, Val F1: 0.7939, Val Acc: 0.8141\n",
      "LR updated from 1.981258e-05 to 1.981208e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [370], Loss: 0.340492, LR: 1.981208e-05, Train F1: 0.8061, Train Acc: 0.8263, Val F1: 0.7939, Val Acc: 0.8144\n",
      "LR updated from 1.981208e-05 to 1.981157e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 12 epoch(s).\n",
      "Epoch [371], Loss: 0.345921, LR: 1.981157e-05, Train F1: 0.8071, Train Acc: 0.8278, Val F1: 0.7955, Val Acc: 0.8164\n",
      "LR updated from 1.981157e-05 to 1.981106e-05\n",
      "No improvement in Val F1 for 13 epoch(s).\n",
      "Epoch [372], Loss: 0.348666, LR: 1.981106e-05, Train F1: 0.8055, Train Acc: 0.8273, Val F1: 0.7941, Val Acc: 0.8162\n",
      "LR updated from 1.981106e-05 to 1.981056e-05\n",
      "No improvement in Val F1 for 14 epoch(s).\n",
      "Epoch [373], Loss: 0.340840, LR: 1.981056e-05, Train F1: 0.8092, Train Acc: 0.8291, Val F1: 0.7973, Val Acc: 0.8173\n",
      "LR updated from 1.981056e-05 to 1.981005e-05\n",
      "** New best model found! Val F1 improved to 0.7973. Model saved.\n",
      "Epoch [374], Loss: 0.342084, LR: 1.981005e-05, Train F1: 0.8090, Train Acc: 0.8286, Val F1: 0.7962, Val Acc: 0.8158\n",
      "LR updated from 1.981005e-05 to 1.980954e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [375], Loss: 0.341314, LR: 1.980954e-05, Train F1: 0.8101, Train Acc: 0.8298, Val F1: 0.7982, Val Acc: 0.8182\n",
      "LR updated from 1.980954e-05 to 1.980904e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "** New best model found! Val F1 improved to 0.7982. Model saved.\n",
      "Epoch [376], Loss: 0.341072, LR: 1.980904e-05, Train F1: 0.8089, Train Acc: 0.8284, Val F1: 0.7957, Val Acc: 0.8153\n",
      "LR updated from 1.980904e-05 to 1.980853e-05\n",
      "No improvement in Val F1 for 1 epoch(s).\n",
      "Epoch [377], Loss: 0.345738, LR: 1.980853e-05, Train F1: 0.8074, Train Acc: 0.8283, Val F1: 0.7946, Val Acc: 0.8158\n",
      "LR updated from 1.980853e-05 to 1.980802e-05\n",
      "No improvement in Val F1 for 2 epoch(s).\n",
      "Epoch [378], Loss: 0.339805, LR: 1.980802e-05, Train F1: 0.8051, Train Acc: 0.8271, Val F1: 0.7927, Val Acc: 0.8147\n",
      "LR updated from 1.980802e-05 to 1.980752e-05\n",
      "No improvement in Val F1 for 3 epoch(s).\n",
      "Epoch [379], Loss: 0.334895, LR: 1.980752e-05, Train F1: 0.8087, Train Acc: 0.8291, Val F1: 0.7970, Val Acc: 0.8174\n",
      "LR updated from 1.980752e-05 to 1.980701e-05\n",
      "No improvement in Val F1 for 4 epoch(s).\n",
      "Epoch [380], Loss: 0.336654, LR: 1.980701e-05, Train F1: 0.8081, Train Acc: 0.8297, Val F1: 0.7962, Val Acc: 0.8183\n",
      "LR updated from 1.980701e-05 to 1.980650e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 5 epoch(s).\n",
      "Epoch [381], Loss: 0.340319, LR: 1.980650e-05, Train F1: 0.8073, Train Acc: 0.8279, Val F1: 0.7943, Val Acc: 0.8152\n",
      "LR updated from 1.980650e-05 to 1.980599e-05\n",
      "No improvement in Val F1 for 6 epoch(s).\n",
      "Epoch [382], Loss: 0.343757, LR: 1.980599e-05, Train F1: 0.8076, Train Acc: 0.8284, Val F1: 0.7953, Val Acc: 0.8163\n",
      "LR updated from 1.980599e-05 to 1.980549e-05\n",
      "No improvement in Val F1 for 7 epoch(s).\n",
      "Epoch [383], Loss: 0.340576, LR: 1.980549e-05, Train F1: 0.8070, Train Acc: 0.8276, Val F1: 0.7949, Val Acc: 0.8158\n",
      "LR updated from 1.980549e-05 to 1.980498e-05\n",
      "No improvement in Val F1 for 8 epoch(s).\n",
      "Epoch [384], Loss: 0.342684, LR: 1.980498e-05, Train F1: 0.8085, Train Acc: 0.8285, Val F1: 0.7962, Val Acc: 0.8164\n",
      "LR updated from 1.980498e-05 to 1.980447e-05\n",
      "No improvement in Val F1 for 9 epoch(s).\n",
      "Epoch [385], Loss: 0.341850, LR: 1.980447e-05, Train F1: 0.8107, Train Acc: 0.8296, Val F1: 0.7977, Val Acc: 0.8170\n",
      "LR updated from 1.980447e-05 to 1.980397e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 10 epoch(s).\n",
      "Epoch [386], Loss: 0.337019, LR: 1.980397e-05, Train F1: 0.8097, Train Acc: 0.8287, Val F1: 0.7972, Val Acc: 0.8163\n",
      "LR updated from 1.980397e-05 to 1.980346e-05\n",
      "No improvement in Val F1 for 11 epoch(s).\n",
      "Epoch [387], Loss: 0.339010, LR: 1.980346e-05, Train F1: 0.8067, Train Acc: 0.8274, Val F1: 0.7952, Val Acc: 0.8160\n",
      "LR updated from 1.980346e-05 to 1.980295e-05\n",
      "No improvement in Val F1 for 12 epoch(s).\n",
      "Epoch [388], Loss: 0.334950, LR: 1.980295e-05, Train F1: 0.8074, Train Acc: 0.8285, Val F1: 0.7955, Val Acc: 0.8171\n",
      "LR updated from 1.980295e-05 to 1.980245e-05\n",
      "No improvement in Val F1 for 13 epoch(s).\n",
      "Epoch [389], Loss: 0.330851, LR: 1.980245e-05, Train F1: 0.8037, Train Acc: 0.8264, Val F1: 0.7916, Val Acc: 0.8141\n",
      "LR updated from 1.980245e-05 to 1.980194e-05\n",
      "No improvement in Val F1 for 14 epoch(s).\n",
      "Epoch [390], Loss: 0.335185, LR: 1.980194e-05, Train F1: 0.8097, Train Acc: 0.8296, Val F1: 0.7977, Val Acc: 0.8178\n",
      "LR updated from 1.980194e-05 to 1.980143e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 15 epoch(s).\n",
      "Epoch [391], Loss: 0.341242, LR: 1.980143e-05, Train F1: 0.8080, Train Acc: 0.8278, Val F1: 0.7955, Val Acc: 0.8155\n",
      "LR updated from 1.980143e-05 to 1.980093e-05\n",
      "No improvement in Val F1 for 16 epoch(s).\n",
      "Epoch [392], Loss: 0.335328, LR: 1.980093e-05, Train F1: 0.8070, Train Acc: 0.8280, Val F1: 0.7953, Val Acc: 0.8165\n",
      "LR updated from 1.980093e-05 to 1.980042e-05\n",
      "No improvement in Val F1 for 17 epoch(s).\n",
      "Epoch [393], Loss: 0.336755, LR: 1.980042e-05, Train F1: 0.8058, Train Acc: 0.8271, Val F1: 0.7933, Val Acc: 0.8147\n",
      "LR updated from 1.980042e-05 to 1.979992e-05\n",
      "No improvement in Val F1 for 18 epoch(s).\n",
      "Epoch [394], Loss: 0.337333, LR: 1.979992e-05, Train F1: 0.8105, Train Acc: 0.8302, Val F1: 0.7981, Val Acc: 0.8181\n",
      "LR updated from 1.979992e-05 to 1.979941e-05\n",
      "No improvement in Val F1 for 19 epoch(s).\n",
      "Epoch [395], Loss: 0.335907, LR: 1.979941e-05, Train F1: 0.8095, Train Acc: 0.8301, Val F1: 0.7973, Val Acc: 0.8182\n",
      "LR updated from 1.979941e-05 to 1.979890e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 20 epoch(s).\n",
      "Epoch [396], Loss: 0.341071, LR: 1.979890e-05, Train F1: 0.8089, Train Acc: 0.8294, Val F1: 0.7966, Val Acc: 0.8177\n",
      "LR updated from 1.979890e-05 to 1.979840e-05\n",
      "No improvement in Val F1 for 21 epoch(s).\n",
      "Epoch [397], Loss: 0.339489, LR: 1.979840e-05, Train F1: 0.8097, Train Acc: 0.8307, Val F1: 0.7976, Val Acc: 0.8187\n",
      "LR updated from 1.979840e-05 to 1.979789e-05\n",
      "No improvement in Val F1 for 22 epoch(s).\n",
      "Epoch [398], Loss: 0.334496, LR: 1.979789e-05, Train F1: 0.8102, Train Acc: 0.8297, Val F1: 0.7978, Val Acc: 0.8176\n",
      "LR updated from 1.979789e-05 to 1.979738e-05\n",
      "No improvement in Val F1 for 23 epoch(s).\n",
      "Epoch [399], Loss: 0.330759, LR: 1.979738e-05, Train F1: 0.8097, Train Acc: 0.8303, Val F1: 0.7973, Val Acc: 0.8177\n",
      "LR updated from 1.979738e-05 to 1.979688e-05\n",
      "No improvement in Val F1 for 24 epoch(s).\n",
      "Epoch [400], Loss: 0.338682, LR: 1.979688e-05, Train F1: 0.8085, Train Acc: 0.8294, Val F1: 0.7963, Val Acc: 0.8179\n",
      "LR updated from 1.979688e-05 to 1.979637e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 25 epoch(s).\n",
      "Epoch [401], Loss: 0.331969, LR: 1.979637e-05, Train F1: 0.8091, Train Acc: 0.8289, Val F1: 0.7962, Val Acc: 0.8163\n",
      "LR updated from 1.979637e-05 to 1.979586e-05\n",
      "No improvement in Val F1 for 26 epoch(s).\n",
      "Epoch [402], Loss: 0.328987, LR: 1.979586e-05, Train F1: 0.8104, Train Acc: 0.8304, Val F1: 0.7977, Val Acc: 0.8181\n",
      "LR updated from 1.979586e-05 to 1.979536e-05\n",
      "No improvement in Val F1 for 27 epoch(s).\n",
      "Epoch [403], Loss: 0.336798, LR: 1.979536e-05, Train F1: 0.8096, Train Acc: 0.8292, Val F1: 0.7968, Val Acc: 0.8168\n",
      "LR updated from 1.979536e-05 to 1.979485e-05\n",
      "No improvement in Val F1 for 28 epoch(s).\n",
      "Epoch [404], Loss: 0.333893, LR: 1.979485e-05, Train F1: 0.8094, Train Acc: 0.8305, Val F1: 0.7966, Val Acc: 0.8180\n",
      "LR updated from 1.979485e-05 to 1.979434e-05\n",
      "No improvement in Val F1 for 29 epoch(s).\n",
      "Epoch [405], Loss: 0.330977, LR: 1.979434e-05, Train F1: 0.8099, Train Acc: 0.8293, Val F1: 0.7970, Val Acc: 0.8169\n",
      "LR updated from 1.979434e-05 to 1.979384e-05\n",
      "Checkpoint saved to ./models_2k/checkpoint_last_2k.pth\n",
      "No improvement in Val F1 for 30 epoch(s).\n",
      "Early stopping triggered (no F1 improvement).\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 7) Training Loop\n",
    "# -------------------------------------------------\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_iterations = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # 7a) 1000 training iters\n",
    "    # -------------------------\n",
    "    for images, labels in train_loader:\n",
    "        if num_iterations >= num_iterations_per_epoch:\n",
    "            break  # Stop after 1000 iterations = \"1 epoch\"\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_iterations += 1\n",
    "\n",
    "    # Compute average loss for this \"epoch\" (1000 iters)\n",
    "    epoch_loss = running_loss / num_iterations_per_epoch\n",
    "\n",
    "    # -------------------------\n",
    "    # 7b) Evaluate on Train & Val\n",
    "    # -------------------------\n",
    "    train_f1, train_acc = evaluate(model, train_loader, device)\n",
    "    val_f1, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "    # Current LR (assuming single LR group)\n",
    "    # Print current learning rate (assuming all parameter groups have same LR)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch [{epoch+1}], Loss: {epoch_loss:.6f}, LR: {current_lr:.6e}, \"\n",
    "          f\"Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 7c) Scheduler Step\n",
    "    # -------------------------\n",
    "    scheduler.step()  # Decay LR\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"LR updated from {current_lr:.6e} to {new_lr:.6e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 7d) Save a Full Checkpoint periodically\n",
    "    # -------------------------\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch + 1,  # Next epoch to continue from\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_val_f1\": best_val_f1,\n",
    "            \"stagnant_epochs\": stagnant_epochs,\n",
    "        }\n",
    "        torch.save(checkpoint_data, checkpoint_path) # \"./models_2k/checkpoint_last_2k.pth\"\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 7e) Early Stopping & Best Model Saving\n",
    "    #     (Based on best Val F1)\n",
    "    # -------------------------\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        stagnant_epochs = 0\n",
    "\n",
    "        # Save just the state_dict\n",
    "        torch.save(model.state_dict(), best_model_state_dict_path) # \"./models_2k/resnet18_pretrained_2k.pth\"\n",
    "\n",
    "        # Save the full model (less common, but can be convenient)\n",
    "        torch.save(model, best_model_full_path) # \"./models_2k/resnet18_pretrained_full_2k.pth\"\n",
    "\n",
    "        print(f\"** New best model found! Val F1 improved to {best_val_f1:.4f}. Model saved.\")\n",
    "    else:\n",
    "        stagnant_epochs += 1\n",
    "        print(f\"No improvement in Val F1 for {stagnant_epochs} epoch(s).\")\n",
    "\n",
    "        if stagnant_epochs >= patience:\n",
    "            print(\"Early stopping triggered (no F1 improvement).\")\n",
    "            break\n",
    "        \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates F1 score, precision, recall, top 1/ 5/ 10 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test Evaluation Metrics]\n",
      "  F1 Score       : 0.7956\n",
      "  Accuracy       : 0.8195\n",
      "  Precision      : 0.8053\n",
      "  Recall         : 0.8171\n",
      "  Top-5 Accuracy : 0.9963\n",
      "  Top-10 Accuracy: 0.9994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dellio/anaconda3/envs/torch13/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Create test dataset/loader\n",
    "test_dataset = FontDataset(csv_file=test_csv, base_dir=base_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8) Load best saved model for inference on val set\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 8a) Option 1: If you saved only the state_dict\n",
    "best_model = ArabicResNet18(num_classes).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_state_dict_path))\n",
    "best_model.eval()\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Variables to accumulate counts for top-K accuracy\n",
    "    total_samples = 0\n",
    "    correct_top5 = 0\n",
    "    correct_top10 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)  # [batch_size, num_classes]\n",
    "            \n",
    "            # Top-1 predictions (for F1, Accuracy, Precision, Recall)\n",
    "            preds = outputs.argmax(dim=1)  # [batch_size]\n",
    "            \n",
    "            # Collect predictions and labels (CPU) for F1 etc.\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # -------------------------------\n",
    "            # Top-5 accuracy calculation\n",
    "            # -------------------------------\n",
    "            top5_vals, top5_indices = torch.topk(outputs, k=5, dim=1)  # shape: [batch_size, 5]\n",
    "            # Check if the true label is in the top-5 predicted indices\n",
    "            for i in range(labels.size(0)):\n",
    "                if labels[i].item() in top5_indices[i]:\n",
    "                    correct_top5 += 1\n",
    "                    \n",
    "            # -------------------------------\n",
    "            # Top-10 accuracy calculation\n",
    "            # -------------------------------\n",
    "            top10_vals, top10_indices = torch.topk(outputs, k=10, dim=1)  # shape: [batch_size, 10]\n",
    "            # Check if the true label is in the top-10 predicted indices\n",
    "            for i in range(labels.size(0)):\n",
    "                if labels[i].item() in top10_indices[i]:\n",
    "                    correct_top10 += 1\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute global metrics (precision, recall, F1, accuracy)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Compute top-5 and top-10 accuracy\n",
    "    top5_acc = correct_top5 / total_samples\n",
    "    top10_acc = correct_top10 / total_samples\n",
    "\n",
    "    return f1, acc, precision, recall, top5_acc, top10_acc\n",
    "\n",
    "test_f1, test_acc, test_precision, test_recall, test_top5, test_top10 = evaluate(best_model, test_loader, device)\n",
    "\n",
    "print(\"\\n[Test Evaluation Metrics]\")\n",
    "print(f\"  F1 Score       : {test_f1:.4f}\")\n",
    "print(f\"  Accuracy       : {test_acc:.4f}\")\n",
    "print(f\"  Precision      : {test_precision:.4f}\")\n",
    "print(f\"  Recall         : {test_recall:.4f}\")\n",
    "print(f\"  Top-5 Accuracy : {test_top5:.4f}\")\n",
    "print(f\"  Top-10 Accuracy: {test_top10:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
